{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNKd6YVvx9qUIETiW8FR/A2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lazarus-GS/NOMA-FYP-MATLAB/blob/main/ML%20Model/RLnoma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LVhdN1PBiLY",
        "outputId": "ba4575bc-8781-465e-b301-8386ed2ebafb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NearUserPos FarUserPos  NearUserDist  FarUserDist  TransmitPower  SNR  \\\n",
            "0          d4         a1     35.355339    77.781746           -114  -30   \n",
            "1          d4         a1     35.355339    77.781746           -112  -28   \n",
            "2          d4         a1     35.355339    77.781746           -110  -26   \n",
            "3          d4         a1     35.355339    77.781746           -108  -24   \n",
            "4          d4         a1     35.355339    77.781746           -106  -22   \n",
            "\n",
            "   SumRateFarUser  SumRateNearUser  BERFarUser  BERNearUser  \\\n",
            "0    4.780000e-07     4.780000e-07      0.5063       0.5002   \n",
            "1    7.630000e-07     7.630000e-07      0.5005       0.5054   \n",
            "2    1.200000e-06     1.200000e-06      0.4946       0.5033   \n",
            "3    1.940000e-06     1.940000e-06      0.4947       0.4975   \n",
            "4    3.040000e-06     3.040000e-06      0.4987       0.5061   \n",
            "\n",
            "   OutageProbFarUser  OutageProbNearUser OptimalMA     Score  \\\n",
            "0                1.0                 1.0       OMA  0.004980   \n",
            "1                1.0                 1.0       OMA  0.005865   \n",
            "2                1.0                 1.0       OMA  0.008611   \n",
            "3                1.0                 1.0       OMA  0.010022   \n",
            "4                1.0                 1.0       OMA  0.006368   \n",
            "\n",
            "   NormalizedCapacityNear  NormalizedCapacityFar  NormalizedBERNear  \\\n",
            "0            3.500000e-08           3.500000e-08           0.964148   \n",
            "1            8.000000e-08           8.000000e-08           0.974171   \n",
            "2            1.490000e-07           1.490000e-07           0.970123   \n",
            "3            2.660000e-07           2.660000e-07           0.958944   \n",
            "4            4.400000e-07           4.400000e-07           0.975520   \n",
            "\n",
            "   NormalizedBERFar  NormalizedOutageNear  NormalizedOutageFar  \n",
            "0          0.959895                   1.0                  1.0  \n",
            "1          0.943040                   1.0                  1.0  \n",
            "2          0.925894                   1.0                  1.0  \n",
            "3          0.926184                   1.0                  1.0  \n",
            "4          0.937809                   1.0                  1.0  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data_url = \"https://raw.githubusercontent.com/Lazarus-GS/NOMA-FYP-MATLAB/main/New/optimal_ma_dataset.csv\"\n",
        "data = pd.read_csv(data_url)\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Environment and Ground Truth"
      ],
      "metadata": {
        "id": "EuTYMpbCFC5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MANetworkEnv:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.max_distance = 20\n",
        "        self.action_space = 2  # 0: OMA, 1: NOMA\n",
        "\n",
        "    def reset(self):\n",
        "        near_user_dist = np.random.uniform(0, self.max_distance)\n",
        "        far_user_dist = np.random.uniform(0, self.max_distance)\n",
        "        snr = np.random.uniform(-30, 30)  # SNR range from -30dB to 30dB\n",
        "        return [near_user_dist, far_user_dist, snr]\n",
        "\n",
        "    def step(self, action):\n",
        "        # Given an action, compute the reward by looking up the data\n",
        "        state = self.reset()\n",
        "        near_user_dist = state[0]\n",
        "        far_user_dist = state[1]\n",
        "        snr = state[2]\n",
        "\n",
        "        # Fetching the optimal MA from the data based on current state\n",
        "        optimal_ma = self.data.loc[(np.isclose(self.data['NearUserDist'], near_user_dist, atol=0.5)) &\n",
        "                                   (np.isclose(self.data['FarUserDist'], far_user_dist, atol=0.5)) &\n",
        "                                   (np.isclose(self.data['SNR'], snr, atol=0.5))]['OptimalMA']\n",
        "\n",
        "        # If we get a direct match in our data\n",
        "        if not optimal_ma.empty:\n",
        "            optimal_ma = optimal_ma.values[0]\n",
        "        else:\n",
        "            # No direct match, so consider any suitable heuristic or default to OMA or NOMA as required\n",
        "            optimal_ma = 0  # This can be adjusted\n",
        "\n",
        "        if action == optimal_ma:\n",
        "            reward = 1\n",
        "        else:\n",
        "            reward = -1  # Penalize wrong decisions\n",
        "\n",
        "        return state, reward, False  # False indicates not done\n"
      ],
      "metadata": {
        "id": "wvgaOfQQCjKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qlearning Agent"
      ],
      "metadata": {
        "id": "yfJotGbtFGyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, n_actions, learning_rate=0.01, discount_factor=0.9, exploration_rate=1.0, exploration_decay_rate=0.995):\n",
        "        self.n_actions = n_actions\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = discount_factor\n",
        "        self.epsilon = exploration_rate\n",
        "        self.epsilon_decay = exploration_decay_rate\n",
        "        self.q_table = {}\n",
        "\n",
        "    def get_action(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.choice(self.n_actions)\n",
        "        return self.get_best_action(state)\n",
        "\n",
        "    def get_best_action(self, state):\n",
        "        state_key = tuple(state)\n",
        "        if state_key not in self.q_table:\n",
        "            return np.random.choice(self.n_actions)\n",
        "        return np.argmax(self.q_table[state_key])\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        state_key = tuple(state)\n",
        "        next_state_key = tuple(next_state)\n",
        "\n",
        "        if state_key not in self.q_table:\n",
        "            self.q_table[state_key] = np.zeros(self.n_actions)\n",
        "        if next_state_key not in self.q_table:\n",
        "            self.q_table[next_state_key] = np.zeros(self.n_actions)\n",
        "\n",
        "        # Q-learning update rule\n",
        "        best_next_action = np.argmax(self.q_table[next_state_key])\n",
        "        self.q_table[state_key][action] += self.lr * (reward + self.gamma * self.q_table[next_state_key][best_next_action] - self.q_table[state_key][action])\n",
        "\n",
        "        # Decay the exploration rate\n",
        "        self.epsilon *= self.epsilon_decay"
      ],
      "metadata": {
        "id": "9TEZf4FgFIyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(agent, env, episodes=1000):\n",
        "    total_rewards = []\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.get_action(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            agent.update(state, action, reward, next_state)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "        total_rewards.append(total_reward)\n",
        "        if episode % 100 == 0:\n",
        "            print(f\"Episode: {episode}, Total Reward: {total_reward}\")\n",
        "    return total_rewards"
      ],
      "metadata": {
        "id": "1wZreIzGFVPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the environment\n",
        "env = MANetworkEnv(data)\n",
        "\n",
        "# Create the agent\n",
        "agent = QLearningAgent(n_actions=2)\n",
        "\n",
        "# Train the agent\n",
        "rewards = train(agent, env, episodes=10000)\n",
        "\n",
        "# Plot the rewards to observe learning\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(rewards)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Total Rewards\")\n",
        "plt.title(\"Training Progress\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "epaXYs0uFYC2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}